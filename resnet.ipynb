{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np\n%matplotlib inline\nimport matplotlib.pyplot as plt\nfrom PIL import Image\nimport torch\nfrom torchvision import datasets, models, transforms\nimport torch.nn as nn\nfrom torch.nn import functional as F\nimport torch.optim as optim\nimport torchvision\nfrom torch.autograd import Variable\nimport time\nimport copy\nimport cv2 \nimport numpy as np\n\ninput_path = \"../input/data-chamber/DATA_CHAMBER_2021/\" \nuse_gpu = torch.cuda.is_available()","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-11-28T09:44:20.657704Z","iopub.execute_input":"2021-11-28T09:44:20.658523Z","iopub.status.idle":"2021-11-28T09:44:20.670315Z","shell.execute_reply.started":"2021-11-28T09:44:20.658454Z","shell.execute_reply":"2021-11-28T09:44:20.669339Z"},"trusted":true},"execution_count":29,"outputs":[]},{"cell_type":"code","source":"import os\nfrom os import listdir, makedirs, getcwd, remove\nfrom os.path import join, exists, expanduser\n\ncache_dir = expanduser(join('~', '.torch'))\n\nif not exists(cache_dir):\n    makedirs(cache_dir)\nmodels_dir = join(cache_dir, 'models')\nif not exists(models_dir):\n    makedirs(models_dir)\n    \n!cp ../input/pretrained-pytorch-models/* ~/.torch/models/\n!ls ~/.torch/models","metadata":{"execution":{"iopub.status.busy":"2021-11-28T09:44:20.672307Z","iopub.execute_input":"2021-11-28T09:44:20.672666Z","iopub.status.idle":"2021-11-28T09:44:22.912980Z","shell.execute_reply.started":"2021-11-28T09:44:20.672629Z","shell.execute_reply":"2021-11-28T09:44:22.912179Z"},"trusted":true},"execution_count":30,"outputs":[]},{"cell_type":"code","source":"class ImageFolderWithPaths(datasets.ImageFolder):\n    \"\"\"Custom dataset that includes image file paths. Extends\n    torchvision.datasets.ImageFolder\n    \"\"\"\n\n    # override the __getitem__ method. this is the method that dataloader calls\n    def __getitem__(self, index):\n        # this is what ImageFolder normally returns \n        original_tuple = super(ImageFolderWithPaths, self).__getitem__(index)\n        # the image file path\n        path = self.imgs[index][0]\n        # make a new tuple that includes original and the path\n        tuple_with_path = (original_tuple + (path,))\n        return tuple_with_path","metadata":{"execution":{"iopub.status.busy":"2021-11-28T09:44:22.914855Z","iopub.execute_input":"2021-11-28T09:44:22.915131Z","iopub.status.idle":"2021-11-28T09:44:22.921472Z","shell.execute_reply.started":"2021-11-28T09:44:22.915094Z","shell.execute_reply":"2021-11-28T09:44:22.920762Z"},"trusted":true},"execution_count":31,"outputs":[]},{"cell_type":"code","source":"def set_parameter_requires_grad(model, feature_extracting):\n    if feature_extracting:\n        for param in model.parameters():\n            param.requires_grad = False","metadata":{"execution":{"iopub.status.busy":"2021-11-28T09:44:22.923315Z","iopub.execute_input":"2021-11-28T09:44:22.923738Z","iopub.status.idle":"2021-11-28T09:44:24.382981Z","shell.execute_reply.started":"2021-11-28T09:44:22.923707Z","shell.execute_reply":"2021-11-28T09:44:24.382094Z"},"trusted":true},"execution_count":32,"outputs":[]},{"cell_type":"code","source":"def initialize_model(model_name, num_classes, feature_extract = False, use_pretrained=True):\n    # Initialize these variables which will be set in this if statement. Each of these\n    #   variables is model specific.\n    model_ft = None\n    input_size = 0\n\n    if model_name == \"resnet18\":\n        \"\"\" Resnet18\n        \"\"\"\n        model_ft = models.resnet18(pretrained=use_pretrained)\n        set_parameter_requires_grad(model_ft, feature_extract)\n        num_ftrs = model_ft.fc.in_features\n        model_ft.fc = nn.Linear(num_ftrs, num_classes)\n        input_size = 224\n        \n    elif model_name == \"resnet50\":\n        \"\"\" Resnet50\n        \"\"\"\n        model_ft = models.resnet50(pretrained=use_pretrained)\n        set_parameter_requires_grad(model_ft, feature_extract)\n        num_ftrs = model_ft.fc.in_features\n        model_ft.fc = nn.Linear(num_ftrs, num_classes)\n        input_size = 224\n\n    elif model_name == \"alexnet\":\n        \"\"\" Alexnet\n        \"\"\"\n        model_ft = models.alexnet(pretrained=use_pretrained)\n        set_parameter_requires_grad(model_ft, feature_extract)\n        num_ftrs = model_ft.classifier[6].in_features\n        model_ft.classifier[6] = nn.Linear(num_ftrs,num_classes)\n        input_size = 224\n\n    elif model_name == \"vgg19_bn\":\n        \"\"\" VGG16_bn\n        \"\"\"\n        model_ft = models.vgg19_bn().load_state_dict(torch.load(\"../input/vgg19bn/vgg19_bn.pth\"))\n        set_parameter_requires_grad(model_ft, feature_extract)\n        num_features = vgg16.classifier[6].in_features\n        features = list(vgg16.classifier.children())[:-1] # Remove last layer\n        features.extend([nn.Linear(num_features, len(num_classes))]) # Add our layer with 4 outputs\n        vgg16.classifier = nn.Sequential(*features) # Replace the model classifier\n        input_size = 224\n    elif model_name == \"vgg16_bn\":\n        \"\"\" VGG16_bn\n        \"\"\"\n        model_ft = models.vgg16_bn().load_state_dict(torch.load(\"../input/vgg16bn/vgg16_bn.pth\"))\n        set_parameter_requires_grad(model_ft, feature_extract)\n        num_features = vgg16.classifier[6].in_features\n        features = list(vgg16.classifier.children())[:-1] # Remove last layer\n        features.extend([nn.Linear(num_features, len(num_classes))]) # \n        vgg16.classifier = nn.Sequential(*features) # Replace the model classifier\n        input_size = 224\n\n    elif model_name == \"squeezenet\":\n        \"\"\" Squeezenet\n        \"\"\"\n        model_ft = models.squeezenet1_0(pretrained=use_pretrained)\n        set_parameter_requires_grad(model_ft, feature_extract)\n        model_ft.classifier[1] = nn.Conv2d(512, num_classes, kernel_size=(1,1), stride=(1,1))\n        model_ft.num_classes = num_classes\n        input_size = 224\n\n    elif model_name == \"densenet\":\n        \"\"\" Densenet\n        \"\"\"\n        model_ft = models.densenet121(pretrained=use_pretrained)\n        set_parameter_requires_grad(model_ft, feature_extract)\n        num_ftrs = model_ft.classifier.in_features\n        model_ft.classifier = nn.Linear(num_ftrs, num_classes)\n        input_size = 224\n\n    elif model_name == \"inception\":\n        \"\"\" Inception v3\n        Be careful, expects (299,299) sized images and has auxiliary output\n        \"\"\"\n        model_ft = models.inception_v3(pretrained=use_pretrained)\n        set_parameter_requires_grad(model_ft, feature_extract)\n        # Handle the auxilary net\n        num_ftrs = model_ft.AuxLogits.fc.in_features\n        model_ft.AuxLogits.fc = nn.Linear(num_ftrs, num_classes)\n        # Handle the primary net\n        num_ftrs = model_ft.fc.in_features\n        model_ft.fc = nn.Linear(num_ftrs,num_classes)\n        input_size = 299\n    else:\n        print(\"Invalid model name, exiting...\")\n        exit()\n\n    return model_ft, input_size\n\n# Initialize the model for this run\nmodel_ft, input_size = initialize_model('resnet50',3,feature_extract = False, use_pretrained=True)\n","metadata":{"execution":{"iopub.status.busy":"2021-11-28T09:44:24.386587Z","iopub.execute_input":"2021-11-28T09:44:24.386823Z","iopub.status.idle":"2021-11-28T09:44:24.850529Z","shell.execute_reply.started":"2021-11-28T09:44:24.386797Z","shell.execute_reply":"2021-11-28T09:44:24.849783Z"},"trusted":true},"execution_count":33,"outputs":[]},{"cell_type":"code","source":"def opening(img):\n    kernel = np.ones((5, 5), np.uint8)\n    img_erosion = cv2.erode(img, kernel, iterations=1)\n    img_dilation = cv2.dilate(img_erosion, kernel, iterations=1)\n    return img_dilation\ndef denoise(img):\n    denoise_img = cv2.bilateralFilter(img, 15, 75, 75)\n    return denoise_img\ndef apply_brightness_contrast(input_img, brightness = 0, contrast = 0):\n    if brightness != 0:\n        if brightness > 0:\n            shadow = brightness\n            highlight = 255\n        else:\n            shadow = 0\n            highlight = 255 + brightness\n        alpha_b = (highlight - shadow)/255\n        gamma_b = shadow\n        \n        buf = cv2.addWeighted(input_img, alpha_b, input_img, 0, gamma_b)\n    else:\n        buf = input_img.copy()\n    \n    if contrast != 0:\n        f = 131*(contrast + 127)/(127*(131-contrast))\n        alpha_c = f\n        gamma_c = 127*(1-f)\n        \n        buf = cv2.addWeighted(buf, alpha_c, buf, 0, gamma_c)\n\n    return buf","metadata":{"execution":{"iopub.status.busy":"2021-11-28T09:44:24.851788Z","iopub.execute_input":"2021-11-28T09:44:24.852036Z","iopub.status.idle":"2021-11-28T09:44:24.861690Z","shell.execute_reply.started":"2021-11-28T09:44:24.852004Z","shell.execute_reply":"2021-11-28T09:44:24.860140Z"},"trusted":true},"execution_count":34,"outputs":[]},{"cell_type":"code","source":"from PIL import Image\nfrom matplotlib import cm\ndef preprocessing(img):\n    return Image.fromarray(np.uint8(apply_brightness_contrast(opening(denoise(np.array(img))),0,16)))","metadata":{"execution":{"iopub.status.busy":"2021-11-28T09:44:24.863238Z","iopub.execute_input":"2021-11-28T09:44:24.863509Z","iopub.status.idle":"2021-11-28T09:44:24.874159Z","shell.execute_reply.started":"2021-11-28T09:44:24.863475Z","shell.execute_reply":"2021-11-28T09:44:24.873445Z"},"trusted":true},"execution_count":35,"outputs":[]},{"cell_type":"code","source":"data_transforms = {\n    'train': transforms.Compose([\n        transforms.Resize(input_size),\n        transforms.CenterCrop(input_size),\n        preprocessing,\n        transforms.ToTensor(),\n        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n    ]),\n    'validation': transforms.Compose([\n        transforms.Resize(input_size),\n        transforms.CenterCrop(input_size),\n        preprocessing,\n        transforms.ToTensor(),\n        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n    ])\n}\n\nimage_datasets = {\n    'train': \n    ImageFolderWithPaths(input_path + 'train', data_transforms['train']),\n    'validation': \n    ImageFolderWithPaths(input_path + 'test', data_transforms['validation'])\n}\ndataset_sizes = {x: len(image_datasets[x]) for x in ['train','validation']}\n\ndataloaders = {\n    'train':\n    torch.utils.data.DataLoader(image_datasets['train'],\n                                batch_size=32,\n                                shuffle=True,\n                                num_workers=2),  # for Kaggle\n    'validation':\n    torch.utils.data.DataLoader(image_datasets['validation'],batch_size=32,\n                                shuffle=False,\n                                num_workers=2)  # for Kaggle\n}\nclass_names = image_datasets['train'].classes\n\n# See some statistics\nprint(dataloaders)\nlen(dataloaders['train'])","metadata":{"execution":{"iopub.status.busy":"2021-11-28T09:44:24.875249Z","iopub.execute_input":"2021-11-28T09:44:24.875589Z","iopub.status.idle":"2021-11-28T09:44:25.895285Z","shell.execute_reply.started":"2021-11-28T09:44:24.875552Z","shell.execute_reply":"2021-11-28T09:44:25.894545Z"},"trusted":true},"execution_count":36,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nimport matplotlib.pyplot as plt\nplt.ion()   # interactive mode\n\ndef imshow(inp, title=None):\n    \"\"\"Imshow for Tensor.\"\"\"\n    inp = inp.numpy().transpose((1, 2, 0))\n    mean = np.array([0.485, 0.456, 0.406])\n    std = np.array([0.229, 0.224, 0.225])\n    inp = std * inp + mean\n    inp = np.clip(inp, 0, 1)\n    plt.imshow(inp)\n    if title is not None:\n        plt.title(title)\n    plt.pause(0.001)  # pause a bit so that plots are updated\n\ninputs, classes,_ = next(iter(dataloaders['train']))\n\n# Make a grid from batch\nout = torchvision.utils.make_grid(inputs)\n\nimshow(out)\n","metadata":{"execution":{"iopub.status.busy":"2021-11-28T09:44:25.896702Z","iopub.execute_input":"2021-11-28T09:44:25.897354Z","iopub.status.idle":"2021-11-28T09:44:29.822634Z","shell.execute_reply.started":"2021-11-28T09:44:25.897311Z","shell.execute_reply":"2021-11-28T09:44:29.821735Z"},"trusted":true},"execution_count":37,"outputs":[]},{"cell_type":"code","source":"def train_model(model, criterion, optimizer, scheduler, num_epochs=2, is_inception=False):\n    since = time.time()\n    \n    best_model_wts = copy.deepcopy(model.state_dict())\n    best_acc = 0.0\n    loss_values = []\n    acc_values = []\n    for epoch in range(num_epochs):\n        print('Epoch {}/{}'.format(epoch + 1, num_epochs))\n        print('-' * 10)\n\n        # Each epoch has a training and validation phase\n        for phase in ['train','validation']:\n            train_batches = len(dataloaders[phase])\n            if phase == 'train':\n                model.train()  # Set model to training mode\n            else:\n                model.eval()   # Set model to evaluate mode\n\n            running_loss = 0.0\n            running_corrects = 0\n\n            # Iterate over data.\n            for i,(inputs, labels,_) in enumerate(dataloaders[phase]):\n                print(\"\\rTraining batch {}/{}\".format(i+1, train_batches), end='', flush=True)\n                inputs = inputs.to(device)\n                labels = labels.to(device)\n\n                # zero the parameter gradients\n                optimizer.zero_grad()\n\n                # forward\n                # track history if only in train\n                with torch.set_grad_enabled(phase == 'train'):\n                    if is_inception and phase == 'train':\n                        outputs, aux_outputs = model(inputs)\n                        loss1 = criterion(outputs, labels)\n                        loss2 = criterion(aux_outputs, labels)\n                        loss = loss1 + 0.4*loss2\n                    else:\n                        outputs = model(inputs)\n                        loss = criterion(outputs, labels)\n                    \n                    _, preds = torch.max(outputs, 1)\n\n                    # backward + optimize only if in training phase\n                    if phase == 'train':\n                        loss.backward()\n                        optimizer.step()\n                running_loss += loss.item() * inputs.size(0)\n                running_corrects += torch.sum(preds == labels.data)\n            if phase == 'train':\n                scheduler.step()\n            epoch_loss = running_loss / dataset_sizes[phase]\n            epoch_acc = running_corrects.double() / dataset_sizes[phase]\n            loss_values.append(epoch_loss)\n            acc_values.append(epoch_acc)\n            print('\\n{} Loss: {:.4f} Acc: {:.4f}'.format(\n                phase, epoch_loss, epoch_acc))\n\n            # deep copy the model\n            if phase == 'validation' and epoch_acc > best_acc:\n                best_acc = epoch_acc\n                best_model_wts = copy.deepcopy(model.state_dict())\n\n        print()\n\n    time_elapsed = time.time() - since\n    print('Training complete in {:.0f}m {:.0f}s'.format(\n        time_elapsed // 60, time_elapsed % 60))\n    print('Best val Acc: {:4f}'.format(best_acc))\n\n    # load best model weights\n    model.load_state_dict(best_model_wts)\n    return model,acc_values,loss_values","metadata":{"execution":{"iopub.status.busy":"2021-11-28T09:44:29.826646Z","iopub.execute_input":"2021-11-28T09:44:29.827157Z","iopub.status.idle":"2021-11-28T09:44:29.842307Z","shell.execute_reply.started":"2021-11-28T09:44:29.827110Z","shell.execute_reply":"2021-11-28T09:44:29.841564Z"},"trusted":true},"execution_count":38,"outputs":[]},{"cell_type":"code","source":"def visualize_model(model, num_images=6):\n    was_training = model.training\n    model.eval()\n    images_so_far = 0\n    fig = plt.figure()\n\n    with torch.no_grad():\n        for i, (inputs, labels,_) in enumerate(dataloaders['validation']):\n            inputs = inputs.to(device)\n            labels = labels.to(device)\n\n            outputs = model(inputs)\n            _, preds = torch.max(outputs, 1)\n\n            for j in range(inputs.size()[0]):\n                images_so_far += 1\n                ax = plt.subplot(num_images//2, 2, images_so_far)\n                ax.axis('off')\n                ax.set_title('predicted: {}'.format(class_names[preds[j]]))\n                imshow(inputs.cpu().data[j])\n\n                if images_so_far == num_images:\n                    model.train(mode=was_training)\n                    return\n        model.train(mode=was_training)","metadata":{"execution":{"iopub.status.busy":"2021-11-28T09:44:29.843663Z","iopub.execute_input":"2021-11-28T09:44:29.844003Z","iopub.status.idle":"2021-11-28T09:44:29.856307Z","shell.execute_reply.started":"2021-11-28T09:44:29.843968Z","shell.execute_reply":"2021-11-28T09:44:29.855589Z"},"trusted":true},"execution_count":39,"outputs":[]},{"cell_type":"code","source":"def test_model(model, criterion, optimizer):\n    labels_input=list()\n    labels_output=list()\n    vid_id = list()\n    for phase in ['validation']:\n        model.eval()\n\n        running_loss = 0.0\n        running_corrects = 0\n\n        for inputs, labels, fname in dataloaders[phase]:\n            inputs = inputs.to(device)\n            labels = labels.to(device)\n            labels_input= labels_input + labels.tolist()\n            for f in fname:\n                vid_id.append(f.split('/')[-1].split('.')[0].split('_')[0])\n            outputs = model(inputs)\n            loss = criterion(outputs, labels)\n            _, preds = torch.max(outputs, 1)\n            \n            labels_output= labels_output + preds.tolist()\n    return labels_input,labels_output,vid_id\n            ","metadata":{"execution":{"iopub.status.busy":"2021-11-28T09:44:29.857630Z","iopub.execute_input":"2021-11-28T09:44:29.858022Z","iopub.status.idle":"2021-11-28T09:44:29.866942Z","shell.execute_reply.started":"2021-11-28T09:44:29.857971Z","shell.execute_reply":"2021-11-28T09:44:29.866211Z"},"trusted":true},"execution_count":40,"outputs":[]},{"cell_type":"code","source":"from torch.optim import lr_scheduler\n\ndevice = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n\nmodel_ft = model_ft.to(device)\n\n# Observe that only parameters of final layer are being optimized as\n# opposed to before.\nparams_to_update = []\nfor name,param in model_ft.named_parameters():\n    if param.requires_grad == True:\n        params_to_update.append(param)\n\noptimizer_conv = optim.SGD(params_to_update, lr=0.01, momentum=0.9)\n\ncriterion = nn.CrossEntropyLoss()\n# Decay LR by a factor of 0.1 every epoch\nexp_lr_scheduler = lr_scheduler.StepLR(optimizer_conv, step_size=3, gamma=0.1)","metadata":{"execution":{"iopub.status.busy":"2021-11-28T09:44:29.868228Z","iopub.execute_input":"2021-11-28T09:44:29.868840Z","iopub.status.idle":"2021-11-28T09:44:29.917322Z","shell.execute_reply.started":"2021-11-28T09:44:29.868804Z","shell.execute_reply":"2021-11-28T09:44:29.916682Z"},"trusted":true},"execution_count":41,"outputs":[]},{"cell_type":"code","source":"model_ft,acc,loss = train_model(model_ft, criterion, optimizer_conv,\n                         exp_lr_scheduler, num_epochs=3, is_inception=False) # As an example, only show the results of 2 epoch","metadata":{"execution":{"iopub.status.busy":"2021-11-28T09:44:29.918533Z","iopub.execute_input":"2021-11-28T09:44:29.919232Z","iopub.status.idle":"2021-11-28T09:55:57.037063Z","shell.execute_reply.started":"2021-11-28T09:44:29.919178Z","shell.execute_reply":"2021-11-28T09:55:57.036154Z"},"trusted":true},"execution_count":42,"outputs":[]},{"cell_type":"code","source":"def plot_history(history,loss,acc): \n    fig, ax1 = plt.subplots()\n    \n    ax1.plot(loss, 'r', label=\"loss\")\n    ax1.grid(True)\n    ax1.set_xlabel('epoch')\n    ax1.set_ylabel('loss', color='r')\n    ax1.legend(loc=\"lower right\", fontsize=9)    \n    ax1.tick_params('y', colors='r')\n\n    ax2 = ax1.twinx()\n    ax2.plot(acc, 'g', label=\"acc\")\n    ax2.legend(loc=\"upper right\", fontsize=9)\n    ax2.set_ylabel('accuracy', color='g')        \n    ax2.tick_params('y', colors='g')","metadata":{"execution":{"iopub.status.busy":"2021-11-28T09:55:57.038864Z","iopub.execute_input":"2021-11-28T09:55:57.039748Z","iopub.status.idle":"2021-11-28T09:55:57.047280Z","shell.execute_reply.started":"2021-11-28T09:55:57.039701Z","shell.execute_reply":"2021-11-28T09:55:57.046546Z"},"trusted":true},"execution_count":43,"outputs":[]},{"cell_type":"code","source":"plot_history(model_ft,loss,acc)","metadata":{"execution":{"iopub.status.busy":"2021-11-28T09:55:57.048380Z","iopub.execute_input":"2021-11-28T09:55:57.049423Z","iopub.status.idle":"2021-11-28T09:55:57.383963Z","shell.execute_reply.started":"2021-11-28T09:55:57.049362Z","shell.execute_reply":"2021-11-28T09:55:57.383236Z"},"trusted":true},"execution_count":44,"outputs":[]},{"cell_type":"code","source":"y_true,y_pred,vid_id = test_model(model_ft, criterion, optimizer_conv)","metadata":{"execution":{"iopub.status.busy":"2021-11-28T09:55:57.385287Z","iopub.execute_input":"2021-11-28T09:55:57.385540Z","iopub.status.idle":"2021-11-28T09:56:39.660707Z","shell.execute_reply.started":"2021-11-28T09:55:57.385507Z","shell.execute_reply":"2021-11-28T09:56:39.659846Z"},"trusted":true},"execution_count":45,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import accuracy_score\naccuracy_score(y_true,y_pred)","metadata":{"execution":{"iopub.status.busy":"2021-11-28T09:56:39.662268Z","iopub.execute_input":"2021-11-28T09:56:39.662536Z","iopub.status.idle":"2021-11-28T09:56:40.494011Z","shell.execute_reply.started":"2021-11-28T09:56:39.662498Z","shell.execute_reply":"2021-11-28T09:56:40.493159Z"},"trusted":true},"execution_count":46,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\nimport glob\n\n\ndf = pd.DataFrame(list(zip(y_true,y_pred,vid_id)),columns =['y_true','y_pred','vid_id'])\ndf.to_csv('df.csv',encoding='utf-8',index=False)\n\nvid_list = list(set(df['vid_id'].values))\ny_true = []\ny_pred = []\nfor vid in vid_list:\n    #print(vid)\n    tmp_df = df[df['vid_id']==vid]\n    #print(len(tmp_df))\n    vid_pred = tmp_df['y_pred'].mode().values[0]\n    vid_label = tmp_df['y_true'].mode().values[0]\n    y_true.append(vid_label)\n    y_pred.append(vid_pred)","metadata":{"execution":{"iopub.status.busy":"2021-11-28T09:56:40.495362Z","iopub.execute_input":"2021-11-28T09:56:40.495712Z","iopub.status.idle":"2021-11-28T09:56:40.558782Z","shell.execute_reply.started":"2021-11-28T09:56:40.495674Z","shell.execute_reply":"2021-11-28T09:56:40.558095Z"},"trusted":true},"execution_count":47,"outputs":[]},{"cell_type":"code","source":"accuracy_score(y_true,y_pred)","metadata":{"execution":{"iopub.status.busy":"2021-11-28T09:56:40.560070Z","iopub.execute_input":"2021-11-28T09:56:40.560330Z","iopub.status.idle":"2021-11-28T09:56:40.566233Z","shell.execute_reply.started":"2021-11-28T09:56:40.560294Z","shell.execute_reply":"2021-11-28T09:56:40.565367Z"},"trusted":true},"execution_count":48,"outputs":[]}]}